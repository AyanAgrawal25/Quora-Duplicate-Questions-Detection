{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GFTCJVPlhz17"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jaACwnoi1n6O"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../quora-question-pairs/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "SXLeOoh91qPA",
    "outputId": "9b078a85-1a80-45ef-88a1-09bb0573158d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bTMTBIL_2P1J"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(df['question1'].values.astype(str))+list(df['question2'].values.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_index = {}\n",
    "with open('../quora-question-pairs/glove.840B.300d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        vectors = np.asarray(values[-300:], dtype='float32')\n",
    "        word = ''.join(values[:-300])  \n",
    "        embedding_index[word] = vectors\n",
    "    f.close()\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index)+1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if embedding_index.get(word) is not None:\n",
    "        embedding_matrix[i] = embedding_index.get(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wflmSWTvAamF"
   },
   "outputs": [],
   "source": [
    "def find_embedding(X_train_q1):\n",
    "    l = len(X_train_q1)\n",
    "    X_train_q1_embeddings = np.zeros((0))\n",
    "    for j in range(l):\n",
    "      q11=X_train_q1[j]\n",
    "      q1 = ''.join([m if ord(m) < 128 else ' ' for m in q11])    #remove non ascii\n",
    "    try:\n",
    "            str1=q1.split(\" \")\n",
    "            l1=len(str1)\n",
    "            f1=np.array((300))\n",
    "            for i in range(l1):\n",
    "                word = str1[i]\n",
    "                f1 = np.add(f1,embedding_matrix[word_index[word]])\n",
    "            X_train_q1_embeddings=np.append(X_train_q1_embeddings,f1)\n",
    "    except:\n",
    "            e = sys.exc_info()\n",
    "            print(e)\n",
    "            continue\n",
    "    X_train_q1_embeddings = X_train_q1_embeddings.reshape(l,300)\n",
    "    return  X_train_q1_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FU7JKmy3mvHH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['is_duplicate']\n",
    "X_train,X_test,y_train,y_test = train_test_split(df,y, test_size = float(3/10), random_state =0)\n",
    "X_valid,X_test,y_valid,y_test = train_test_split(X_test,y_test, test_size = float(1/3), random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIwL6pd4myk3"
   },
   "outputs": [],
   "source": [
    "X_train_q1 = X_train.question1.values\n",
    "X_train_q2 = X_train.question2.values\n",
    "X_test_q1 = X_test.question1.values\n",
    "X_test_q2 = X_test.question2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3Uv-wInm01Z"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "X_train_q1_embeddings = np.zeros((len(X_train_q1),300))\n",
    "X_train_q1_embeddings = find_embedding(X_train_q1)\n",
    "X_train_q2_embeddings = np.zeros((len(X_train_q2),300))\n",
    "X_train_q2_embeddings = find_embedding(X_train_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHO3TcvFm6DT"
   },
   "outputs": [],
   "source": [
    "def concatenation(X1,X2):\n",
    "    l=len(X1)\n",
    "    X_train_embeddings = np.zeros((l, 300))\n",
    "    for i in range(l):\n",
    "        X_train_embeddings[i] = np.concatenate((X1[i], X2[i]), axis=0)\n",
    "    return X_train_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnF9X0Jdm61z"
   },
   "outputs": [],
   "source": [
    "X_train_final = concatenation(X_train_q1_embeddings,X_train_q2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYl4HsXfnnzV"
   },
   "outputs": [],
   "source": [
    "X_test_q1_embeddings = np.zeros((len(X_test_q1),300))\n",
    "X_test_q1_embeddings = find_embedding(X_test_q1)\n",
    "X_test_q2_embeddings = np.zeros((len(X_test_q2),300))\n",
    "X_test_q2_embeddings = find_embedding(X_test_q2)\n",
    "X_test_final = concatenation(X_test_q1_embeddings,X_test_q2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crc_tWaf6WzY"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(solver='adam',alpha=1e-5,hidden_layer_sizes=(100),random_state=1)\n",
    "ml=mlp.fit(X_train_final,y_train)\n",
    "y_pred = mlp.predict(X_test_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaxmEYHi8UDQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTUtSc2NnZjf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CBOW_smai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
